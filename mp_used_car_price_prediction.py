# -*- coding: utf-8 -*-
"""MP-Used Car Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1elnvl2Y7FLT0BfMd1_XpACqDPbaWgBpj

# **Used Car Price Prediction**
"""

! pip install --upgrade "kaleido==0.1.*"

# Commented out IPython magic to ensure Python compatibility.
# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import datetime
import plotly.express as px
import plotly.graph_objects as go
import plotly.offline as pyo
import plotly.io as pio
warnings.filterwarnings('ignore')

# %matplotlib inline

"""# Data Loading and Data Exploration

---------

## Load Data

The dataset is from Kaggle [https://www.kaggle.com/datasets/tunguz/used-car-auction-prices/data]
"""

# Import dataset
dfcar = pd.read_csv('https://drive.google.com/uc?id=1KfTnuIeppn-b5bdMyIvVVbII_P7m1Tnu', on_bad_lines="skip")
dfcar.head(3)

"""**Data Dictionary**  
1. Date : The year of production of the cars.  
2. Make : The brand of the car.  
3. Model : The edition of the car of a specific brand.  
4. Trim : The trim levels for a car are just different versions of the model.  
5. Body : The body style of a vehicle refers to the shape and model of a particular car make.  
6. Transmission : The mechanism that moves the power from the engine to the wheels.  
7. VIN : Vehichel identification number.  
8. State : The state in which the car is auctioned.  
9. Condition : The condition of the cars being at the time of auction.  
10. Odometer : The distance the car has travelled since manufactured.  
11. Color : Exterior color of the car.  
12. Interior : Interior color of the car.  
13. Seller : The seller of the car, car dealers.  
14. **mmr : Manhiem Market Report, the market estimated price of the cars. (Target)**
15. sellingprice : The price a car was sold at auctions.  
16. saledate : The date on which the car has been sold.

## Data Exploration
"""

dfcar.info()

# Check null values
dfcar.isnull().sum()

"""1. In total, there are 558,811 entries.
2. The data types are int, float and object, and appear to be in accordance with the columns. However, for the `saledate` column the initial data type is 'object' needs to be changed to datetime.
3. There are 9 features that have Null values, there are `make`, `model`, `trim`, `body`, `transmission`, `condition`, `odometer`, `color`, `interior`. There needs to be follow-up on Data Cleaning.
"""

# Check duplicated rows
dfcar.duplicated().sum()

"""No duplicated rows."""

# Categorize the features
num = ['year', 'condition', 'odometer', 'mmr', 'sellingprice']
cat = ['make', 'model', 'trim', 'body', 'transmission', 'vin', 'state', 'color', 'interior', 'seller', 'saledate']

# Set display float format
pd.set_option('display.float_format', lambda x: '%.3f' % x)

dfcar[num].describe()

"""1. `condition` has a categorical value between 1-5.
2. The value of the `odometer` appears to have a large distance between min and max where the min value is 1 and the max is almost 1 million. Of course, this is something that seems odd. Apart from that, the mean and median values are also very far apart, namely 16,000.
3. Overall, the `mmr` and `sellingprice` values are not very different. However, the min-max and mean-median values are very different.
4. Some columns in the numerical feature also have null values, so they need to be handled at the next stage.
"""

dfcar[cat].describe()

"""1. From the data above, we can see at a glance what values are the top ones, appear frequently, and how many unique values each feature has.
2. For example, Ford is the most popular brand among a total of 96 other brands. Apart from that, the sedan body is also one of the most favorite.
3. Automatic transmission is more sought after by customers than manual by more than 90%.
4. Black dominates both the exterior and interior colors.

----------

# EDA (Exploratory Data Analysis)

-------

## UNIVARIATE ANALYSIS

### Unique Value From Some Features
"""

# Check unique value 'year'
dfcar['year'].unique()

"""All values are appropriate in the form of numbers starting from 1982 to 2015."""

# Check unique value 'make'
dfcar['make'].unique()

"""There is a null value in the `make` feature. Apart from that, the value of the `make` column is still too diverse, where there are words with capital letters, all upper case, and all lower case. Therefore, we will change the values to all lowercase in the next stage. The same thing might also happen to other object type features."""

# Check unique value 'trim'
dfcar['trim'].unique()

"""The `trim` feature has more than 1000 unique values so only a few of them appear.

### Outliers (year, condition, odometer, MMR, sellingprice)
"""

# Outliers

plt.figure(figsize=(6, 3))
for i in range(0, len(num)):
    plt.subplot(1, len(num), i+1)
    sns.boxplot(dfcar[num[i]], color='red', orient='v')
    plt.title(num[i])
    plt.tight_layout()

"""It can be seen that `mmr` and `sellingprice` have the most outliers, as do the `odometer` and `year` columns. The quartiles and median also seem far away. This can happen because there is a lot of car data (500k++) so the MMR, selling price and odometer values also vary greatly. The column that looks the most normal is the `condition` column.

### Data Distribution (year, condition, odometer, MMR, sellingprice)
"""

def generate_distribution_plot(train_df, continuous_features):
    # create copy of dataframe
    data = train_df[continuous_features].copy()
    # Create subplots
    fig, axes = plt.subplots(nrows=len(data.columns)//2, ncols=3,figsize=(21,7))
    fig.subplots_adjust(hspace=0.7)

    # set fontdict
    font = {'family': 'serif',
        'color':  'darkred',
        'weight': 'normal',
        'size': 16,
        }

    # Generate distplot
    for ax, feature in zip(axes.flatten(), data.columns):
        feature_mean = data[feature].mean()
        feature_median = data[feature].median()
        feature_mode = data[feature].mode().values[0]
        sns.distplot(data[feature],ax=ax)
        ax.set_title(f'Analysis of {feature}', fontdict=font)
        ax.axvline(feature_mean, color='r', linestyle='--', label="Mean")
        ax.axvline(feature_median, color='g', linestyle='--', label="Median")
        ax.axvline(feature_mode, color='b', linestyle='--', label="Mode")
        ax.legend()
    plt.show()

generate_distribution_plot(dfcar, num)

"""From the graph above we can conclude:
1. From the `year` column we can see that sales tend to increase every year and peak sales were between 2010 and 2015. We can also see that the `year` column is left-skewed.
2. In the `condition` column, the condition of the car that is in the range of number 2 is the majority of the other conditions. This shows that the condition of the used car isn't good. However, there are quite a few cars in the condition that are rated 3 or above to 4.
3. For the 'odometer', it can be seen that the majority are in the numbers 0 to 0.2, which means the odometer ranges from 0 to 200k. Meanwhile, using cars over 200k is relatively rare.
4. The `mmr` and `sellingprice` columns look similar.
5. The `odometer`, `mmr`, and `sellingprice` columns are right-skewed.

### Checking Abnormal Values

Like the previous analysis where we found min and max values that seemed unreasonable, we will further check the data containing these values.

The columns are `odometer`, `mmr`, and `sellingprice`.

**1. Min and Max From Odometer**
"""

# Rows with 'odometer' = 1 (min)
min_odo = dfcar[dfcar['odometer'] == 1.000]
min_odo.sample(3)

# Rows with 'odometer' = 999999 (max)
max_odo = dfcar[dfcar['odometer'] == 999999.000]
max_odo.sample(3)

"""From the min and max of the `odometer` column above, it can be seen that the data not only contains min and max values that are far away but other columns are also abnormal. This can be seen from the sample above where the min and max rows also contain many null values, low mmr and selling price values, and poor conditions.

**2. Min from Selling Price**
"""

# Rows with 'sellingprice' = 1 (min)
min_price = dfcar[dfcar['sellingprice'] == 1.000]
min_price

"""**3. Min from MMR**"""

# Rows with 'mmr' = 25 (min)
min_mmr = dfcar[dfcar['mmr'] == 25.000]
min_mmr.sample(3)

"""From the data above, where the sellingprice is 1 and mmr is $25, there are many null values, the `sellingprice` value is relatively small, and the condition isn't good. This data will be handled later at the data cleansing stage.

## MULTIVARIATE ANALYSIS

### Correlation Heatmap
"""

# Correlation Heatmap
corr_matrix = dfcar.corr()
plt.figure(figsize=(7, 4))
sns.heatmap(corr_matrix, annot=True, color='blue', center=0)
plt.title('Correlation Heatmap')
plt.show()

"""1. The correlation between `year` and `sellingprice` is positive, indicating that the newer the car, the higher the selling price.
2. The `condition` column has a positive correlation with `sellingprice`, which means the better the condition of the car, the higher the selling price.
3. The `odometer` column has a negative correlation with `sellingprice`, indicating that the less mileage, the higher the selling price.
4. The `mmr` column has a fairly strong positive correlation with `sellingprice`, indicating that the selling price predicted by MMR has a good correlation with the actual selling price.
5. The 'Year' and 'Odometer' columns also show a fairly high negative correlation, meaning that the latest cars have odometers that tend to be small

### Correlation Between MMR and Selling Price

As the results of the previous analysis where the MMR and Selling Price values are highly similar, we will try to visualize the relationship between them.
"""

from plotnine import *

# MMR and Selling Price
(ggplot(dfcar)
 + aes(x ='mmr', y ='sellingprice')
 + geom_point(color='#f00000')
 + labs(title='MMR vs Selling Price', x='MMR', y='Selling Price')
 + annotate(geom = "segment", x = 0, xend = 200000, y = 0, yend = 200000, color='blue')
)

"""MMR is an abbreviation for Manhiem Market Report, a specific report which is an indicator of wholesale prices for used vehicles. Price calculations are based on more than 10 million sales transactions over the previous 13 months at exact prices by vehicle sales at Manheim auction houses.
So, it can be said that MMR is worthy of being used as a price reference.

----------

# BUSINESS INSIGHT AND VISUALIZATION

Here are some business insight :  

**1. Pricing from MMR**  

**2. The Growth of Transaction Amount and Number of Transaction**  

**3. Top Brand**

##1. MMR as the Target
First we will use dataset copy to separate insight mining and modeling.
Then, look at how sales are and divide the car data into 2 categories (based on targets) : Above MMR and Below MMR.
"""

# Copy dataset
copy = dfcar.copy()

# 'is_sold_below_mmr' (1=Yes, 0=No)
copy['is_sold_below_mmr'] = copy.apply(lambda x: 1 if x.sellingprice < x.mmr else 0, axis=1)

# Sales Percentage
transaction = copy.groupby('is_sold_below_mmr').agg({'vin':'count'}).rename({'vin':'total_transaction'}, axis=1)
transaction['percentage'] = round(((transaction/transaction.sum())*100), 2)
transaction

copy.head(3)

copy['Price_Status'] = copy['sellingprice'] > copy['mmr']
sns.set(style='whitegrid')

plt.figure(figsize=(10, 5))
sns.scatterplot(data=copy, x='mmr', y='sellingprice', hue='Price_Status', palette={True: 'red', False: 'blue'})
plt.xlabel('MMR')
plt.ylabel('Selling Price')
plt.title('Selling Price vs MMR')
plt.legend(title='Price Status', loc='upper left', labels=['Below MMR', 'Above MMR'])

plt.tight_layout()
plt.show()

"""## 2. The Growth of Transaction Amount and Number of Transactions"""

# Slicing the saledate
timeget = []
for i in copy['saledate'].str.split():
    time_split = i[:5]
    date_string = ' '.join(time_split)
    timeget.append(date_string)
copy['datetime'] =  pd.to_datetime(timeget)
copy['month_year'] = copy.datetime.dt.strftime('%b-%y')

copy['date'] = copy['datetime'].dt.date
copy['date'] = pd.to_datetime(copy['date'], format = '%Y-%m-%d')

# Grouping monthly sale 'month_year'
monthly_sale_summary = copy.groupby(['month_year']).agg({'vin' : 'count', 'sellingprice' : 'sum'}).reset_index()
monthly_sale_summary.columns = ['Date', 'Total_Transaction', 'Transaction_Amount']
# Sorting Date
custom_dict = {'Jan-14':0, 'Feb-14':1, 'Dec-14':2, 'Jan-15':3, 'Feb-15':4, 'Mar-15':5, 'Apr-15':6, 'May-15':7, 'Jun-15':8, 'Jul-15':9}
monthly_sale_summary = monthly_sale_summary.sort_values(by=['Date'], key=lambda x: x.map(custom_dict))
monthly_sale_summary

from plotly.subplots import make_subplots
fig = go.Figure()
trace1 = go.Scatter(x=monthly_sale_summary.Date, y=monthly_sale_summary.Transaction_Amount, name='Amount in USD($)')
trace2 = go.Bar(x=monthly_sale_summary['Date'], y=monthly_sale_summary['Total_Transaction'], marker_color='#b51a18', name='Total Transactions')

fig.update_xaxes(title_text="Date")
fig.update_yaxes(title_text="Transactions")

fig = make_subplots(specs=[[{"secondary_y": True}]])
fig.add_trace(trace1, secondary_y=True)
fig.add_trace(trace2)

annotations = []
annotations.append(dict(xref='paper', yref='paper', x=0.424, y=0.95, text='Most Transaction', font =dict(size=15), showarrow=True, arrowhead=1, arrowsize=1, arrowwidth=2))

fig['layout'].update(height = 600, width = 1100, title = "The Growth of Transaction", title_font_size=34, plot_bgcolor='#fcf9d9', annotations=annotations)
fig.show('png')

"""The graph above shows that February 2015 was the peak of transactions with a total of around 160k transactions and total revenue of around $2 billion. In contrast to January-February 2014 and also in April and July 2015 with the least transactions.

## 3. Top Brand
"""

# Brand
sum_make = copy.groupby('make')['sellingprice'].sum().reset_index()
sum_make = sum_make.sort_values(by='sellingprice', ascending=False)

top_10 = sum_make.head(10)
fig = go.Figure()
fig.add_trace(go.Bar(x=top_10['make'], y=top_10['sellingprice'], marker_color='#f72877'))
fig.update_layout(title='Top 10 Brand by Sum of Selling Price',
                  title_font_size=30,
                  xaxis_title='Brand',
                  yaxis_title='Sum of Selling Price in USD ($)',
                  plot_bgcolor='black',
                  paper_bgcolor='#f5dae8',
                  font=dict(color='black'))
fig.show('png')

"""Ford is the most popular brand with total revenues of almost $1.4B, indicating that Ford has a strong market. Then there was a significant decline in the second popular car, Chevrolet. Meanwhile, Chevrolet, Nissan, Toyota, etc. show a constant difference or the gap is not too big.

# Data Preparation

----

## Data Cleansing

### Dropping

**Duplicated Rows**

In the previous Data Exploration stage, there were no duplicate rows so they didn't need to be handled.

**Rows with Abnormal Values**
"""

dfcar2 = dfcar.copy()

# Drop nilai min odo
i_min_odo = dfcar2[((dfcar2.odometer == 1.000))].index
dfcar2 = dfcar2.drop(i_min_odo)

# Drop nilai max odo
i_max_odo = dfcar2[((dfcar2.odometer == 999999.000))].index
dfcar2 = dfcar2.drop(i_max_odo)

# Drop nilai min sellingprice
i_min_price = dfcar2[((dfcar2.sellingprice == 1.000))].index
dfcar2 = dfcar2.drop(i_min_price)

# Drop nilai min mmr
i_min_mmr = dfcar2[((dfcar2.mmr == 25.000))].index
dfcar2 = dfcar2.drop(i_min_mmr)

dfcar2.shape

"""**Irrelevant Features**

Some features seem less relevant and their unique value is too broad, as we have analyzed previously. So it is assumed that these features do not have a particular pattern. These features are `vin`, `sellingprice`, and `saledate` which will be removed. The reasons are:  
a. `vin` is not a feature that determines the price of a car because it's juts the identification number for the car.  
b. `sellingprice` has a high correlation (0.98) with `mmr` (target feature).  
c. `saledate` does not show a significant factor because the time is too broad and uncertain so there is no special pattern that influences the selling price of a car.
"""

# Delete the irrelevant features
dfcar2 = dfcar2.drop(['vin','sellingprice','saledate'], axis=1)
dfcar2.head(3)

"""### Handle Unique Values

In some features there are many unique values as previously explored. We will handle unique values into:  
1. All categorical data except saledate will be changed to lower case to get the same unique value.  
2. The " — " value in the `color` and `interior` features will be replaced with the mode value, that is black.
"""

def content_consistent(df):
    cols = df.select_dtypes(object).columns.difference(['saledate'])
    df[cols] = df[cols].apply(lambda x: x.str.lower())
    return df

dfcar2 = content_consistent(dfcar2)

import re

# Replace color and interior
dfcar2['color'].replace('—','black',inplace=True)
dfcar2['interior'].replace('—','black',inplace=True)

# Replace model dan trim by remove special characters
dfcar2['model'].replace(to_replace='[^A-Za-z0-9 ]+', value='', regex=True, inplace=True)
dfcar2['trim'].replace(to_replace='[^A-Za-z0-9 ]+', value='', regex=True, inplace=True)
dfcar2['seller'].replace(to_replace='[^A-Za-z0-9 ]+', value='', regex=True, inplace=True)

# Replace same meaning of value
dfcar2['make'].replace('landrover','land rover',inplace=True)
dfcar2['make'].replace('mercedes-b','mercedes-benz',inplace=True)
dfcar2['make'].replace('mercedes','mercedes-benz',inplace=True)
dfcar2['make'].replace('vw','volkswagen',inplace=True)
dfcar2['make'].replace('ford tk','ford truck',inplace=True)
dfcar2['body'].replace('koup','coupe',inplace=True)
dfcar2['body'].replace('regular-cab','regular cab',inplace=True)
dfcar2['body'].replace('xtracab','extended cab',inplace=True)

# View the results of handling value and column inconsistencies
c = ['make', 'model', 'trim', 'body', 'color', 'interior', 'state', 'seller']

dfcar2[c].describe()

"""After cleaning the data by generalizing the string writing, the results are visible:  
a. `make` which was originally 96 now only has 61 unique values  
b. `model` which was originally 973 now only has 841 unique values  
c. `trim` which was originally 1975 now only has 1883 unique values  
d. `body` which was originally 86 now only has 42 unique values  
e. `seller` which was originally 14264 now only has 14152 unique values  
f. `color` and `interior` are reduced by 1 value from the previous unique value because they have been replaced by the mode value (black).  
g. `state` is the only one that still has the same value.

### Handle Missing Values

There are 9 features that have Null values, they are `make`, `model`, `trim`, `body`, `transmission`, `condition`, `odometer`, `color`, `interior`.  

1. All categorical data will be filled with mode.  
2. `condition` which has a fairly normal distribution of data, will be replaced by mean.  
3. `odometer` which only has 94 null values, will be deleted.
"""

# Imputation of null values with mode
mode_value = dfcar2.filter(['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']).mode()
cols = ['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']

dfcar2[cols] = dfcar2[cols].fillna(dfcar.mode().iloc[0])

# Imputation of null values with mean
dfcar2['condition'] = dfcar2['condition'].fillna(dfcar2['condition'].mean())

# Removes remaining null values
dfcar2.dropna(axis=0, inplace=True)
dfcar2.isnull().sum()

"""### Handle Outliers"""

df_clean = dfcar2.copy()

# Function to check for outliers
def outlier_del(df, column, mode):
    q1 = df.iloc[:,column].quantile(0.25)
    q3 = df.iloc[:,column].quantile(0.75)
    iqr = q3-q1
    lower_tail = q1 - (1.5 * iqr)
    upper_tail = q3 + (1.5 * iqr)
    column_name = df.columns[column]
    total_outliers = df[(df.iloc[:,column] <= lower_tail)|(df.iloc[:,column] >= upper_tail)].iloc[:,column].count()
    total_row = df.iloc[:,column].count()
    percent_outliers = round(((total_outliers/total_row)*100),2)
    if mode == 'summary':
        return print('Total Outliers of ', column_name, ' :', total_outliers, ' and outliers percentage:', percent_outliers, '%')
    elif mode == 'df':
        return df[(df.iloc[:,column] >= lower_tail)&(df.iloc[:,column] <= upper_tail)]
    else :
        return print('Check the Input')

# Check the total outliers
column = [0, 7, 8, 12]

for i in range(0, len(column)):
    outlier_del(df_clean, column[i], 'summary')

# Delete outliers
df_clean = df_clean[df_clean.index.isin(outlier_del(dfcar2, 0, 'df').reset_index()['index'])]
df_clean = df_clean[df_clean.index.isin(outlier_del(dfcar2, 8, 'df').reset_index()['index'])]
df_clean = df_clean[df_clean.index.isin(outlier_del(dfcar2, 12, 'df').reset_index()['index'])]
df_clean.shape

# Create a plot to compare before and after removing outliers
plt.figure(figsize=(8, 3))
plt.subplot(1, 2, 1)
sns.distplot(dfcar2.year)
plt.title('Year - Outliers')
plt.subplot(1, 2, 2)
sns.distplot(df_clean.year)
plt.title('Year - No Outliers')
plt.show()

# Skewness before handle outliers
print(f"Before - Skewness Coefficient of Year : {round(dfcar2.year.skew(), 3)}")

# Skewness after handle outliers
print(f"After - Skewness Coefficient of Year : {round(df_clean.year.skew(), 3)}")

plt.figure(figsize=(8, 3))
plt.subplot(1, 2, 1)
sns.distplot(dfcar2.odometer)
plt.title('Odometer - Outliers')
plt.subplot(1, 2, 2)
sns.distplot(df_clean.odometer)
plt.title('Odometer - No Outliers')
plt.show()

# Skewness before handle outliers
print(f"Before - Skewness Coefficient of Odometer : {round(dfcar2.odometer.skew(), 3)}")

# Skewness after handle outliers
print(f"After - Skewness Coefficient of Odometer : {round(df_clean.odometer.skew(), 3)}")

plt.figure(figsize=(8, 3))
plt.subplot(1, 2, 1)
sns.distplot(dfcar2.mmr)
plt.title('MMR - Outliers')
plt.subplot(1, 2, 2)
sns.distplot(df_clean.mmr)
plt.title('MMR - No Outliers')
plt.show()

# Skewness before handle outliers
print(f"Before - Skewness Coefficient of MMR : {round(dfcar2.mmr.skew(), 3)}")

# Skewness after handle outliers
print(f"After - Skewness Coefficient of MMR : {round(df_clean.mmr.skew(), 3)}")

"""## Feature Engineering

### Feature Extraction

Because the dataset contains many categorical features, we need to convert them first into numbers. But before that, we need to carry out feature extraction to see patterns that can simplify the feature encoding.

#### 1. Overall
Overall rating of the car, grouped by the mean of MMR from `condition` and `odometer`.
"""

df_yearcond = df_clean.groupby(['condition', 'odometer']).agg({'mmr' : ['mean','median']})
df_yearcond.describe()

def segment(x):
    if x['odometer'] <= 7200:
        segment = 'Poor'
    elif x['odometer'] > 7200 and x['odometer'] <= 12150:
        segment = 'Fair'
    elif x['odometer'] > 12150 and x['odometer'] <= 17600:
        segment = 'Good'
    else:
        segment = 'Very Good'
    return segment

df_clean['Overall'] = df_clean.apply(lambda x: segment(x), axis=1)

"""#### 2. Made In

Simplify the value in the `make` feature by categorizing car brands according to the country of origin.
"""

# Car brand groups based on country of origin
us_made = ['chevrolet', 'ford', 'buick', 'cadillac', 'jeep', 'dodge', 'chrysler', 'ram', 'scion', 'pontiac', 'saturn', 'mercury', 'hummer', 'gmc', 'gmc truck',
           'oldsmobile', 'ford truck', 'lincoln', 'plymouth', 'airstream']
germany_made = ['bmw', 'audi', 'mercedes-benz', 'porsche', 'smart', 'chev truck', 'volkswagen']
japan_made = ['nissan', 'acura', 'lexus', 'infiniti', 'mitsubishi', 'mazda', 'toyota', 'subaru', 'honda', 'suzuki', 'isuzu', 'mazda tk']
uk_made = ['mini', 'land rover', 'jaguar']
italy_made = ['fiat', 'maserati']
korea_made = ['kia', 'hyundai', 'hyundai tk', 'daewoo']
swedia_made = ['volvo', 'saab']

made_in = []

for maker in df_clean['make']:
    if maker in us_made:
        country = 'US'
    elif maker in germany_made:
        country = 'DEU'
    elif maker in japan_made:
        country = 'JPN'
    elif maker in uk_made:
        country = 'UK'
    elif maker in italy_made:
        country = 'ITA'
    elif maker in korea_made:
        country = 'KOR'
    else:
        country = 'SWE'

    made_in.append(country)

df_clean['made_in'] = made_in

"""#### 3. Top Make
Brand segmentation based on 11 top brands. The rest of them will be labelled as 'Other'.
"""

# Top Brand based on Year of making
top_make = df_clean.groupby('make')['year'].count().reset_index().sort_values('year', ascending=False)
top_make = top_make.iloc[:11]
top_make.drop('year', axis=1, inplace=True)
top_make['top_make'] = top_make['make']
top_make

df_clean = df_clean.merge(top_make, how='left', on='make')
df_clean['top_make'].fillna('Other', inplace=True)
df_clean['top_make'].unique()

"""### Feature Selection"""

# Select the features
df_clean = df_clean.drop(['make', 'transmission'], axis=1)
df_clean.head(3)

"""### Feature Encoding

#### One-Hot Encoding
"""

df_pre = df_clean.copy()

# One Hot Encoding for Categorical Features
df_pre = pd.get_dummies(df_pre, columns=['Overall', 'made_in', 'top_make'])
df_pre.head(3)

# New Correlation Heatmap
corr_matrix = df_pre.corr()
f, ax = plt.subplots(figsize=(20, 20))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn',annot_kws={'size': 9}, ax=ax)
plt.show()

"""#### Target Encoding
Target encoding is performed on several features to maintain important information contained in category features, especially in the context of target predictions.
"""

pip install --upgrade category_encoders

from category_encoders import TargetEncoder

# Target encoding
cols = ['model', 'trim', 'body', 'state']
target = 'mmr'

encoder = TargetEncoder()
df_pre[cols] = encoder.fit_transform(df_pre[cols], df_pre[target])

"""#### Count Encoding
This method is used for categorical features where each category is replaced by its frequency of occurrence.
"""

# Count encoding for 'seller'
count_encoding_seller = df_pre['seller'].value_counts().to_dict()
df_pre['seller'] = df_pre['seller'].map(count_encoding_seller)

# Count encoding for 'color'
count_encoding_seller = df_pre['color'].value_counts().to_dict()
df_pre['color'] = df_pre['color'].map(count_encoding_seller)

# Count encoding for'interior'
count_encoding_seller = df_pre['interior'].value_counts().to_dict()
df_pre['interior'] = df_pre['interior'].map(count_encoding_seller)

"""## Train-Test Split"""

df_new = df_pre.copy()

from sklearn.model_selection import train_test_split

# Split the data
features = df_new.select_dtypes(["float64", "int64", "boolean", "uint8"]).columns
x = df_new[features].drop('mmr', axis=1)
y = df_new.mmr

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

"""### Pre-Processing"""

from sklearn.preprocessing import StandardScaler

# Scaling
scaler    = StandardScaler()
n = ['year', 'odometer', 'condition']
xtrain[n] = scaler.fit_transform(xtrain[n])
xtest[n] = scaler.transform(xtest[n])

xtrain.describe()

"""# Modelling and Evaluation

## Modelling
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Function to display model evaluation scores
def eval_regression(model, y_pred, xtrain, xtest, ytrain,  ytest):
    print("MAE: " , mean_absolute_error(ytest, y_pred))
    print("RMSE: " , mean_squared_error(ytest, y_pred, squared=False))
    print('R2 score: ', r2_score(ytest, y_pred))
    print('Coefficients: ', regressor.coef_)
    print('Intercept: ', regressor.intercept_)

"""### Linear Regression"""

from sklearn.linear_model import LinearRegression

# Train model linear regression
regressor = LinearRegression()
regressor.fit(xtrain, ytrain)

# Predict
y_pred = regressor.predict(xtest)

# Show the evaluation
eval_regression(regressor, y_pred, xtrain, xtest, ytrain, ytest)

"""### Lasso Regression"""

from sklearn.linear_model import Lasso

# Training model lasso regression
lasso = Lasso(alpha=0.1, fit_intercept=True)
lasso.fit(xtrain, ytrain)

# Predict
y_pred = lasso.predict(xtest)

eval_regression(lasso, y_pred, xtrain, xtest, ytrain, ytest)

"""### Ridge Regression"""

from sklearn.linear_model import Ridge

# Training model ridge regression
ridge = Ridge(alpha=1.0)
ridge.fit(xtrain, ytrain)

# Predict
y_pred = ridge.predict(xtest)

eval_regression(ridge, y_pred, xtrain, xtest, ytrain, ytest)

"""### Random Forest Regression"""

from sklearn.ensemble import RandomForestRegressor

# Training model random forest
rf = RandomForestRegressor(random_state=104, verbose=False)
rf.fit(xtrain, ytrain)

# Predict
ytrain_pred = rf.predict(xtrain)
ytest_pred = rf.predict(xtest)

def evaluasi(model, ytrain_pred, ytest_pred, xtrain, xtest, ytrain,  ytest):
    print("MAE train: " , mean_absolute_error(ytrain, ytrain_pred))
    print("MAE test: " , mean_absolute_error(ytest, ytest_pred))
    print("RMSE train: " , mean_squared_error(ytrain, ytrain_pred, squared=False))
    print("RMSE test: " , mean_squared_error(ytest, ytest_pred, squared=False))
    print('R2 score train: ', r2_score(ytrain, ytrain_pred))
    print('R2 score test: ', r2_score(ytest, ytest_pred))

evaluasi(rf, ytrain_pred, ytest_pred, xtrain, xtest, ytrain, ytest)

"""### CatBoost Regressor"""

pip install catboost

from catboost import CatBoostRegressor

# Training model catboost
catboost = CatBoostRegressor(verbose=False)
catboost.fit(xtrain, ytrain)

# Predict
ytrain_pred = catboost.predict(xtrain)
ytest_pred = catboost.predict(xtest)

evaluasi(catboost, ytrain_pred, ytest_pred, xtrain, xtest, ytrain, ytest)

"""### XGBoost"""

import xgboost as xgb

# Training model xgboost
xgb_regressor = xgb.XGBRegressor()
xgb_regressor.fit(xtrain, ytrain)

# Predict
ytrain_pred = xgb_regressor.predict(xtrain)
ytest_pred = xgb_regressor.predict(xtest)

evaluasi(xgb_regressor, ytrain_pred, ytest_pred, xtrain, xtest, ytrain, ytest)

"""The best model is the **Random Forest** model because it has the highest R2 score and the smallest MAE and RMSE among the other models.

## Evaluation

### Feature Importance
"""

# Displays a feature importance graph
feat_importances = pd.Series(rf.feature_importances_, index=x.columns)
ax = feat_importances.nlargest(15).plot(kind='barh')
ax.invert_yaxis()
plt.xlabel('score')
plt.ylabel('feature')
plt.title('Feature Importance Score')

"""### Cross Validation"""

from sklearn.model_selection import cross_validate

# Cross Validation
SEED = 101
np.random.seed(SEED)

model_cv = RandomForestRegressor(verbose=False)
scores = cross_validate(model_cv, xtrain, ytrain, cv = 5, return_train_score=False)
media = scores['test_score'].mean()
std_dev = scores['test_score'].std()
print("Accuracy with cross validation, 5 = [%.2f%%, %.2f%%]" % ((media - 2 * std_dev)*100, (media + 2 * std_dev) * 100))

"""------------

## New Selling Price
"""

# Real Price X Predict Price
df_pred = pd.DataFrame(columns=['mmr', 'sellingprice_predicted'])
df_pred['mmr'] = ytest
rf_model = RandomForestRegressor(verbose=False)
rf_model.fit(xtrain, ytrain)
df_pred['sellingprice_predicted'] = rf_model.predict(xtest)
df_pred

"""**Adding Selling Price Prediction to the Original Dataset**"""

y_df = pd.DataFrame(data = df_pred, columns = ['sellingprice_predicted'], index = xtest.index.copy())
df_out = pd.merge(dfcar, y_df, how = 'left', left_index = True, right_index = True)
df_out.head()

df_compare = df_out.copy()

# Removing NaN selling price predictions  (NaN means the unused data for modeling)
df_compare.dropna(subset = ['sellingprice_predicted'], inplace=True)

# Create a new column 'is below mmr' for comparison before-after modelling
df_compare['is_below_mmr'] = df_compare.apply(lambda x: 1 if x.sellingprice < x.mmr else 0, axis=1)
df_compare

"""The data above is the same as that we used during modeling to (105562 rows of data) which the predicted selling price has been added.

**Comparison**

The Original Dataset
"""

df_compare['is_below_mmr'] = df_compare.apply(lambda x: 1 if x.sellingprice < x.mmr else 0, axis=1)

transaction = df_compare.groupby('is_below_mmr').agg({'mmr':'count'}).rename({'mmr':'total_transaction'}, axis=1)
transaction['percentage'] = round(((transaction/transaction.sum())*100), 2)
transaction

# Calculate the average selling price and condition relative to MMR
def displayMeanSP(df):
    spAboveMMR=[]
    spBelowMMR=[]

    for index, row in df.iterrows():
      if row['is_below_mmr'] == 1:
        spBelowMMR.append(row['sellingprice'])
      else:
        spAboveMMR.append(row['sellingprice'])

    print(f'Ori Dataset - Mean of Selling Price above MMR = {sum(spAboveMMR)/len(spAboveMMR)}')
    print(f'Ori Dataset - Mean of Selling Price below MMR = {sum(spBelowMMR)/len(spBelowMMR)}')

displayMeanSP(df_compare)

"""After-Modelling Dataset"""

df_pred['is_below_mmr'] = df_pred.apply(lambda x: 1 if x.sellingprice_predicted < x.mmr else 0, axis=1)

transaction = df_pred.groupby('is_below_mmr').agg({'mmr':'count'}).rename({'mmr':'total_transaction'}, axis=1)
transaction['percentage'] = round(((transaction/transaction.sum())*100), 2)
transaction

# Calculate the average selling price and condition relative to MMR
def displayMeanSP(df):
    spAboveMMR=[]
    spBelowMMR=[]

    for index, row in df.iterrows():
      if row['is_below_mmr'] == 1:
        spBelowMMR.append(row['sellingprice_predicted'])
      else:
        spAboveMMR.append(row['sellingprice_predicted'])

    print(f'Model - Mean of Selling Price above MMR = {sum(spAboveMMR)/len(spAboveMMR)}')
    print(f'Model- Mean of Selling Price below MMR = {sum(spBelowMMR)/len(spBelowMMR)}')

displayMeanSP(df_pred)

"""# Conclusion

1. With a sample of 20%, the price prediction made shows that the percentage of prices below mmr is 48.19%, where this figure shows a decrease of around 3% from the initial data and prices above mmr also have an increase of 3% compared to the initial data.  

2. After modeling, the average price under MMR is 13468. If we compare to the average price of cars sold under MMR from original dataset which is 12422, we can conclude that the car price recommended for sale is $1046 (8.4%) higher, so sellers can get more profits.

3. The same thing goes to the average car above mmr is 12959 compared to the original dataset which is 14702. We can recommend that seller can sell the car for $2041 (11.8%) lower so that the price is not too expensive.
"""